{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-based solver for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will create a **gradient descent** solver for **ridge regression** and then compare it to the built-in solver in `sklearn.linear_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up notebook and create data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading in some standard packages, we create a synthetic data set consisting of data points `(x,y)`:\n",
    "* `x`: 100-dimensional vector whose coordinates are independent draws from a standard normal (Gaussian) distribution\n",
    "* `y`: response value given by `y = wx + e` where `w` is a target regression function and `e` is Gaussian noise\n",
    "\n",
    "We will fix `w` to be the 100-dimensional vector whose first ten coordinates are exactly 1.0, and whose remaining coordinates are zero. Thus only the first ten coordinates of `x` are relevant to the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the purpose of generate_data is to pull down some data points from a normal distribution and assign a new relationship to them \n",
    "## the assigned relation is the possibility is only related to the first 10 parameter and e\n",
    "## what we need to do is to find this assigned relationship of [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,.......]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following procedure, **generate_data**, creates a data set of a specified number of points. It is invoked as follows:\n",
    "* `trainx, trainy = generate_data(n)`\n",
    "\n",
    "Here:\n",
    "* `n` is the target number of points\n",
    "* `trainx`: `nx100` array of data points\n",
    "* `trainy`: array of `n` response values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n):\n",
    "    d = 100\n",
    "    w = np.zeros(d)\n",
    "    for i in range(0,10):\n",
    "        w[i] = 1.0\n",
    "    \n",
    "    trainx = np.random.normal(size=(n,d))\n",
    "    e = np.random.normal(size=(n))\n",
    " \n",
    "    trainy = np.dot(trainx, w) + e\n",
    "    #\n",
    "    return trainx, trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h =1\n",
    "f = 10\n",
    "w = np.zeros(f)\n",
    "for i in range(0,10):\n",
    "    w[i] = 1.0\n",
    "trainxx = np.random.normal(size=(h,f))\n",
    "e = np.random.normal(size=(h))\n",
    " \n",
    "trainyy = np.dot(trainxx, w) + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(trainxx) #default value is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(trainxx, ddof=1) #defalt value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = []\n",
    "#loss = (np.dot(ones,y-(np.dot(x,w) + b)))**2 + C*np.dot(w,w)  \n",
    "#losses. append(loss)    \n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient descent solver for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to do:**</font> Define a procedure, **ridge_regression_GD**, that uses gradient descent to solve the ridge regression problem. It is invoked as follows:\n",
    "\n",
    "* `w,b,losses = ridge_regression_GD(x,y,C)`\n",
    "\n",
    "Here, the input consists of:\n",
    "* training data `x,y`, where `x` and `y` are numpy arrays of dimension `n`-by-`d` and `n`, respectively (if there are `n` training points)\n",
    "* regularization constant `C`\n",
    "\n",
    "The function should find the `d`-dimensional vector `w` and offset `b` that minimize the ridge regression loss function (with regularization constant `C`), and return:\n",
    "* `w` and `b`\n",
    "* `losses`, an array containing the ridge regression loss at each iteration\n",
    "\n",
    "<font color=\"magenta\">Advice:</font> First figure out the derivative, which has a relatively simple form. Next, when implementing gradient descent, think carefully about two issues.\n",
    "\n",
    "1. What is the step size?\n",
    "2. When has the procedure converged?\n",
    "\n",
    "Take the time to experiment with different ways of handling these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_GD(x,y,C):\n",
    "    ### Put your code here\n",
    "    j = 0\n",
    "    k = 0\n",
    "    m = 0\n",
    "    eta = 0.001\n",
    "    b = np.zeros(len(y))\n",
    "    w= np.zeros(x[1].size)\n",
    "    # errors = np.zeros(len(y))\n",
    "\n",
    "    \n",
    "    errors = y-(np.dot(x,w)+b)\n",
    "    ones = np.ones(len(y))\n",
    "    loss = np.dot(errors.T,errors) + C*np.dot(w.T,w) \n",
    "    losses.append(loss)    \n",
    "    derivate = [-2*np.dot(errors.T,ones),-2*np.dot(errors.T,x)+2*C*w]    \n",
    "    w = w - eta * derivate [1]\n",
    "    b = b - eta * derivate [0]   \n",
    "    #loss = np.dot(ones,(y-(np.dot(x,w) + b))**2) + C*np.dot(w,w) \n",
    "   \n",
    "    while losses[m] <=losses[m-j] or m ==1:\n",
    "    \n",
    "        errors = y-(np.dot(x,w)+b)\n",
    "       \n",
    "    \n",
    "        loss = np.dot(errors.T,errors) + C*np.dot(w.T,w) \n",
    "        losses. append(loss)  \n",
    "        \n",
    "        j = 1\n",
    "        m = m + 1\n",
    "        eta = eta/j\n",
    "    \n",
    "        derivate = [-2*np.sum(errors),-2*np.dot(x.T,errors)+2*C*w]     \n",
    "        w = w - eta * derivate [1]\n",
    "        b = b - eta * derivate [0]  \n",
    "        if eta <= 2**(-20):\n",
    "            break\n",
    "\n",
    "        \n",
    "    return w,b,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out and print a graph of the loss values during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1988.8244558011988,\n",
       " 598.3467424505088,\n",
       " 331.72824900302885,\n",
       " 234.74020112420618,\n",
       " 188.2366787904868,\n",
       " 161.44576893853102,\n",
       " 144.01985969681797,\n",
       " 131.77586649706532,\n",
       " 122.7365196492727,\n",
       " 115.83792092550732,\n",
       " 110.44779309550273,\n",
       " 106.16195562912645,\n",
       " 102.70785927234151,\n",
       " 99.89416854534532,\n",
       " 97.58227312891373,\n",
       " 95.66918426091148,\n",
       " 94.0767678987338,\n",
       " 92.74470424256549,\n",
       " 91.6257366893568,\n",
       " 90.68237854554408,\n",
       " 89.88457620048406,\n",
       " 89.20801630176838,\n",
       " 88.63287657537462,\n",
       " 88.14288861673373,\n",
       " 87.72462420697471,\n",
       " 87.3669445592525,\n",
       " 87.06057021932978,\n",
       " 86.79774162248604,\n",
       " 86.57194868095728,\n",
       " 86.37771357777143,\n",
       " 86.2104150249921,\n",
       " 86.0661451586666,\n",
       " 85.94159235238696,\n",
       " 85.83394477889249,\n",
       " 85.7408106987882,\n",
       " 85.66015231981906,\n",
       " 85.59023072740396,\n",
       " 85.52955989221573,\n",
       " 85.47686815255165,\n",
       " 85.43106587616154,\n",
       " 85.39121824852312,\n",
       " 85.35652232731285,\n",
       " 85.32628765718906,\n",
       " 85.29991986339151,\n",
       " 85.27690674343143,\n",
       " 85.25680645819425,\n",
       " 85.23923749086951,\n",
       " 85.22387009720686,\n",
       " 85.21041901598312,\n",
       " 85.19863724607926,\n",
       " 85.18831072766655,\n",
       " 85.17925379084994,\n",
       " 85.17130525665462,\n",
       " 85.16432509322647,\n",
       " 85.15819154516694,\n",
       " 85.15279866653776,\n",
       " 85.14805419866826,\n",
       " 85.14387774280995,\n",
       " 85.14019918519465,\n",
       " 85.1369573383875,\n",
       " 85.13409876818258,\n",
       " 85.13157677981822,\n",
       " 85.12935054112864,\n",
       " 85.12738432350397,\n",
       " 85.12564684429624,\n",
       " 85.12411069666024,\n",
       " 85.12275185482005,\n",
       " 85.1215492444569,\n",
       " 85.12048436936966,\n",
       " 85.11954098680134,\n",
       " 85.11870482488737,\n",
       " 85.1179633365903,\n",
       " 85.11730548526312,\n",
       " 85.11672155765207,\n",
       " 85.11620300072134,\n",
       " 85.11574227917387,\n",
       " 85.1153327509665,\n",
       " 85.11496855847966,\n",
       " 85.11464453331622,\n",
       " 85.11435611297435,\n",
       " 85.11409926787059,\n",
       " 85.11387043739234,\n",
       " 85.11366647383113,\n",
       " 85.1134845931988,\n",
       " 85.1133223320588,\n",
       " 85.11317750961702,\n",
       " 85.11304819441467,\n",
       " 85.11293267504875,\n",
       " 85.11282943442151,\n",
       " 85.11273712708156,\n",
       " 85.1126545592764,\n",
       " 85.11258067138304,\n",
       " 85.11251452242679,\n",
       " 85.11245527643253,\n",
       " 85.11240219038736,\n",
       " 85.11235460361858,\n",
       " 85.11231192841709,\n",
       " 85.11227364175605,\n",
       " 85.11223927797425,\n",
       " 85.11220842230813,\n",
       " 85.11218070517265,\n",
       " 85.11215579710128,\n",
       " 85.11213340426775,\n",
       " 85.11211326452086,\n",
       " 85.11209514387198,\n",
       " 85.1120788333826,\n",
       " 85.11206414640495,\n",
       " 85.1120509161348,\n",
       " 85.11203899344002,\n",
       " 85.11202824493343,\n",
       " 85.11201855126124,\n",
       " 85.11200980558309,\n",
       " 85.11200191222065,\n",
       " 85.11199478545691,\n",
       " 85.11198834846758,\n",
       " 85.11198253237075,\n",
       " 85.11197727538095,\n",
       " 85.11197252205561,\n",
       " 85.11196822262413,\n",
       " 85.11196433238977,\n",
       " 85.11196081119641,\n",
       " 85.11195762295321,\n",
       " 85.11195473521013,\n",
       " 85.11195211877951,\n",
       " 85.11194974739759,\n",
       " 85.11194759742271,\n",
       " 85.11194564756542,\n",
       " 85.11194387864694,\n",
       " 85.11194227338362,\n",
       " 85.11194081619354,\n",
       " 85.11193949302415,\n",
       " 85.11193829119708,\n",
       " 85.11193719926985,\n",
       " 85.11193620691152,\n",
       " 85.11193530479142,\n",
       " 85.11193448447938,\n",
       " 85.11193373835596,\n",
       " 85.1119330595322,\n",
       " 85.1119324417775,\n",
       " 85.11193187945472,\n",
       " 85.11193136746188,\n",
       " 85.11193090117993,\n",
       " 85.11193047642556,\n",
       " 85.11193008940904,\n",
       " 85.11192973669588,\n",
       " 85.11192941517267,\n",
       " 85.11192912201616,\n",
       " 85.11192885466541,\n",
       " 85.11192861079687,\n",
       " 85.11192838830144,\n",
       " 85.11192818526438,\n",
       " 85.11192799994683,\n",
       " 85.11192783076922,\n",
       " 85.11192767629608,\n",
       " 85.11192753522288,\n",
       " 85.11192740636352,\n",
       " 85.11192728863931,\n",
       " 85.11192718106912,\n",
       " 85.11192708276016,\n",
       " 85.11192699289997,\n",
       " 85.11192691074889,\n",
       " 85.11192683563345,\n",
       " 85.11192676694026,\n",
       " 85.11192670411052,\n",
       " 85.11192664663513,\n",
       " 85.11192659404999,\n",
       " 85.11192654593209,\n",
       " 85.11192650189574,\n",
       " 85.11192646158916,\n",
       " 85.11192642469148,\n",
       " 85.11192639090997,\n",
       " 85.11192635997743,\n",
       " 85.11192633165008,\n",
       " 85.11192630570528,\n",
       " 85.11192628193982,\n",
       " 85.11192626016803,\n",
       " 85.11192624022044,\n",
       " 85.11192622194207,\n",
       " 85.11192620519148,\n",
       " 85.1119261898393,\n",
       " 85.1119261757673,\n",
       " 85.11192616286738,\n",
       " 85.11192615104082,\n",
       " 85.11192614019716,\n",
       " 85.11192613025375,\n",
       " 85.11192612113508,\n",
       " 85.11192611277194,\n",
       " 85.11192610510102,\n",
       " 85.11192609806446,\n",
       " 85.11192609160922,\n",
       " 85.11192608568675,\n",
       " 85.11192608025266,\n",
       " 85.11192607526628,\n",
       " 85.11192607069037,\n",
       " 85.11192606649082,\n",
       " 85.11192606263639,\n",
       " 85.11192605909842,\n",
       " 85.11192605585077,\n",
       " 85.11192605286932,\n",
       " 85.11192605013211,\n",
       " 85.11192604761897,\n",
       " 85.11192604531139,\n",
       " 85.11192604319243,\n",
       " 85.11192604124652,\n",
       " 85.11192603945946,\n",
       " 85.11192603781818,\n",
       " 85.11192603631065,\n",
       " 85.11192603492593,\n",
       " 85.11192603365394,\n",
       " 85.11192603248544,\n",
       " 85.11192603141193,\n",
       " 85.11192603042566,\n",
       " 85.11192602951952,\n",
       " 85.11192602868687,\n",
       " 85.11192602792178,\n",
       " 85.1119260272187,\n",
       " 85.11192602657262,\n",
       " 85.11192602597882,\n",
       " 85.11192602543312,\n",
       " 85.11192602493153,\n",
       " 85.11192602447052,\n",
       " 85.11192602404681,\n",
       " 85.11192602365725,\n",
       " 85.1119260232992,\n",
       " 85.11192602296998,\n",
       " 85.11192602266739,\n",
       " 85.11192602238917,\n",
       " 85.11192602213335,\n",
       " 85.11192602189814,\n",
       " 85.11192602168188,\n",
       " 85.11192602148304,\n",
       " 85.1119260213002,\n",
       " 85.11192602113205,\n",
       " 85.11192602097742,\n",
       " 85.11192602083523,\n",
       " 85.11192602070444,\n",
       " 85.11192602058415,\n",
       " 85.11192602047352,\n",
       " 85.1119260203718,\n",
       " 85.11192602027822,\n",
       " 85.11192602019213,\n",
       " 85.11192602011293,\n",
       " 85.11192602004013,\n",
       " 85.11192601997308,\n",
       " 85.11192601991146,\n",
       " 85.11192601985476,\n",
       " 85.11192601980258,\n",
       " 85.11192601975459,\n",
       " 85.11192601971044,\n",
       " 85.11192601966982,\n",
       " 85.11192601963246,\n",
       " 85.11192601959803,\n",
       " 85.11192601956641,\n",
       " 85.11192601953728,\n",
       " 85.11192601951049,\n",
       " 85.11192601948584,\n",
       " 85.11192601946314,\n",
       " 85.11192601944228,\n",
       " 85.11192601942304,\n",
       " 85.11192601940536,\n",
       " 85.11192601938907,\n",
       " 85.11192601937414,\n",
       " 85.11192601936037,\n",
       " 85.11192601934768,\n",
       " 85.11192601933597,\n",
       " 85.11192601932524,\n",
       " 85.11192601931536,\n",
       " 85.11192601930625,\n",
       " 85.11192601929788,\n",
       " 85.11192601929014,\n",
       " 85.11192601928308,\n",
       " 85.11192601927655,\n",
       " 85.1119260192705,\n",
       " 85.11192601926497,\n",
       " 85.1119260192599,\n",
       " 85.1119260192552,\n",
       " 85.11192601925086,\n",
       " 85.11192601924688,\n",
       " 85.11192601924324,\n",
       " 85.11192601923987,\n",
       " 85.11192601923678,\n",
       " 85.11192601923389,\n",
       " 85.11192601923128,\n",
       " 85.11192601922885,\n",
       " 85.1119260192266,\n",
       " 85.11192601922458,\n",
       " 85.11192601922268,\n",
       " 85.11192601922097,\n",
       " 85.11192601921934,\n",
       " 85.11192601921789,\n",
       " 85.11192601921648,\n",
       " 85.11192601921526,\n",
       " 85.11192601921411,\n",
       " 85.11192601921304,\n",
       " 85.11192601921206,\n",
       " 85.11192601921118,\n",
       " 85.11192601921033,\n",
       " 85.11192601920958,\n",
       " 85.11192601920887,\n",
       " 85.11192601920823,\n",
       " 85.1119260192076,\n",
       " 85.11192601920709,\n",
       " 85.11192601920658,\n",
       " 85.11192601920612,\n",
       " 85.11192601920568,\n",
       " 85.1119260192053,\n",
       " 85.11192601920493,\n",
       " 85.11192601920459,\n",
       " 85.11192601920429,\n",
       " 85.11192601920399,\n",
       " 85.11192601920375,\n",
       " 85.1119260192035,\n",
       " 85.11192601920328,\n",
       " 85.11192601920305,\n",
       " 85.1119260192029,\n",
       " 85.11192601920271,\n",
       " 85.11192601920256,\n",
       " 85.1119260192024,\n",
       " 85.11192601920226,\n",
       " 85.11192601920214,\n",
       " 85.11192601920203,\n",
       " 85.11192601920192,\n",
       " 85.1119260192018,\n",
       " 85.11192601920173,\n",
       " 85.11192601920163,\n",
       " 85.11192601920159,\n",
       " 85.11192601920152,\n",
       " 85.11192601920143,\n",
       " 85.11192601920138,\n",
       " 85.11192601920133,\n",
       " 85.11192601920129,\n",
       " 85.11192601920123,\n",
       " 85.11192601920119,\n",
       " 85.11192601920115,\n",
       " 85.11192601920112,\n",
       " 85.11192601920106,\n",
       " 85.11192601920106,\n",
       " 85.11192601920101,\n",
       " 85.11192601920101,\n",
       " 85.11192601920096,\n",
       " 85.11192601920098]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses=[]\n",
    "# Generate 200 data points\n",
    "n = 200\n",
    "x,y = generate_data(n)\n",
    "# Set regularization constant\n",
    "C = 1.0\n",
    "\n",
    "# Run gradient descent solver\n",
    "w, b, losses = ridge_regression_GD(x,y,C)\n",
    "# Plot the losses\n",
    "plt.plot(losses,'r')\n",
    "plt.xlabel('Iterations', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.show()\n",
    "len(losses)\n",
    "losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "n = 200\n",
    "x,y = generate_data(n)\n",
    "j = 0\n",
    "k = 0\n",
    "m = 0\n",
    "eta = 0.001\n",
    "b = np.zeros(len(y))\n",
    "w= np.zeros(x[1].size)\n",
    "    # errors = np.zeros(len(y))\n",
    "\n",
    "    \n",
    "errors = y-(np.dot(x,w)+b)\n",
    "ones = np.ones(len(y))\n",
    "loss = np.dot(errors.T,errors) + C*np.dot(w.T,w) \n",
    "losses.append(loss)    \n",
    "derivate = [-2*np.sum(errors),-2*np.dot(x.T,errors)+2*C*w]    \n",
    "w = w - eta * derivate [1]\n",
    "b = b - eta * derivate [0]   \n",
    "    #loss = np.dot(ones,(y-(np.dot(x,w) + b))**2) + C*np.dot(w,w) \n",
    "   \n",
    "losses\n",
    "eta * derivate [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while losses[m] <=losses[m-j] or m ==1:\n",
    "    \n",
    "    errors = y-(np.dot(x,w)+b)\n",
    "       \n",
    "    \n",
    "    loss = np.dot(errors.T,errors) + C*np.dot(w.T,w) \n",
    "    losses. append(loss)  \n",
    "        \n",
    "    j = 1\n",
    "    m = m + 1\n",
    "    eta = eta*.5\n",
    "    \n",
    "    derivate = [-2*np.sum(errors),-2*np.dot(x.T,errors)+2*C*w]     \n",
    "    w = w - eta * derivate [1]\n",
    "    b = b - eta * derivate [0]  \n",
    "    if eta <= 2**(-20):\n",
    "        break\n",
    "losses\n",
    "eta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the gradient descent solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the regressor found by your gradient descent procedure to that returned by the built-in ridge regression solver in `sklearn`. We will compare them in two ways:\n",
    "* Their MSE values\n",
    "* The distance between the corresponding `w`-vectors\n",
    "\n",
    "The latter should be smaller than 10^{-4}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(w,b,x,y):\n",
    "    residuals = y - (np.dot(x, w) + b)\n",
    "    return np.dot(residuals, residuals)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of gradient descent solver:  0.5992490001160715\n",
      "MSE of built-in solver:  0.5992490001155558\n",
      "Distance between w-coefficients:  5.022071253271302e-11\n"
     ]
    }
   ],
   "source": [
    "# Generate 200 data points\n",
    "n = 200\n",
    "x,y = generate_data(n)\n",
    "# Set regularization constant\n",
    "C = 10\n",
    "# Run gradient descent solver and compute its MSE\n",
    "w, b, losses = ridge_regression_GD(x,y,C)\n",
    "# Use built-in routine for ridge regression and compute MSE\n",
    "regr = linear_model.Ridge(alpha=C)\n",
    "regr.fit(x, y)\n",
    "# Print MSE values and L2 distance between the regression functions\n",
    "print \"MSE of gradient descent solver: \", compute_mse(w,b,x,y)\n",
    "print \"MSE of built-in solver: \", mean_squared_error(regr.predict(x), y)\n",
    "# the norm of distance between w from builtin function and w from gradient descent\n",
    "print \"Distance between w-coefficients: \", np.linalg.norm(w-regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**Something to think about**</font>\n",
    "\n",
    "The data was originally generated using a linear function in which only ten of the 100 features (the first ten) were relevant. Does the vector `w` returned by ridge regression correctly identify the relevant features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.01970585e+00,  8.71450355e-01,  9.71231453e-01,  7.73099202e-01,\n",
       "        6.63816302e-01,  1.02863963e+00,  9.12156350e-01,  9.10847338e-01,\n",
       "        8.54227198e-01,  9.05176646e-01, -3.55704379e-02, -1.05789523e-01,\n",
       "       -1.70825517e-01,  5.48639105e-02, -6.65388982e-02,  5.41349801e-02,\n",
       "       -7.75464823e-02,  4.99440381e-02,  2.00428903e-01, -1.25633420e-01,\n",
       "       -5.67965455e-02, -7.93589076e-02,  3.10431256e-04,  1.12480527e-01,\n",
       "        1.60727573e-01, -4.66157754e-02,  1.05296321e-01,  1.08500214e-01,\n",
       "       -1.03642816e-01, -3.32774342e-02, -9.59960366e-03,  7.36714223e-03,\n",
       "        1.42640404e-02,  6.57189996e-02,  8.07356492e-02,  8.51593902e-02,\n",
       "        9.94690764e-02,  5.58178486e-02,  5.11091635e-02,  9.94992838e-03,\n",
       "       -1.69679285e-02, -6.57839575e-02,  1.42952361e-01,  4.14714703e-04,\n",
       "        1.72845739e-01, -1.22516744e-01, -6.38400365e-02,  1.35297956e-01,\n",
       "        2.73045216e-03,  4.92092102e-02,  1.51929200e-02, -3.77311914e-03,\n",
       "       -5.56968625e-02, -1.44041340e-01, -1.28105602e-01,  5.00809489e-02,\n",
       "       -1.75474439e-02,  4.80651658e-02, -4.78112777e-02, -4.16719671e-02,\n",
       "        4.41575885e-02, -7.97715006e-02, -8.59101792e-03,  5.51273571e-02,\n",
       "        2.10264631e-02, -7.41865762e-02, -1.11846064e-01, -7.09679986e-02,\n",
       "       -7.27634817e-02, -3.95345004e-02,  2.07544469e-02, -2.10194093e-01,\n",
       "       -1.34889835e-01,  1.53296509e-02, -7.87239988e-02,  3.09951266e-02,\n",
       "        9.32462306e-02,  2.34573103e-02,  1.23141120e-01,  5.66292522e-02,\n",
       "       -1.81074293e-01, -1.34827424e-01,  1.00971606e-01, -3.51048541e-02,\n",
       "       -1.83272616e-01, -4.65697600e-02, -1.17534721e-02, -1.07464912e-01,\n",
       "       -5.16053576e-02,  1.19624509e-01,  1.15233848e-03, -1.28338265e-02,\n",
       "        3.64786606e-02,  9.80079210e-02, -2.17224401e-02,  7.02905085e-02,\n",
       "       -3.25942946e-02,  7.24900983e-02, -5.56410408e-02, -5.40702374e-02])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71, 84, 80, 12, 53, 72, 81, 54, 19, 45, 66, 87, 11, 28, 61, 21, 74,\n",
       "       16, 65, 68, 67, 14, 41, 46, 20, 52, 98, 99, 88, 58, 25, 85, 59, 69,\n",
       "       10, 83, 29, 96, 94, 56, 40, 91, 86, 30, 62, 51, 22, 43, 90, 48, 31,\n",
       "       39, 32, 50, 73, 70, 64, 77, 75, 92, 60, 57, 49, 17, 55, 38, 15, 13,\n",
       "       63, 37, 79, 33, 95, 97, 34, 35, 76, 93, 36, 82, 26, 27, 23, 89, 78,\n",
       "       47, 42, 24, 44, 18,  4,  3,  8,  1,  9,  7,  6,  2,  0,  5],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_index = np.argsort(w)\n",
    "sort_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the vector w returned by ridge regression correctly identify the relevant features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
